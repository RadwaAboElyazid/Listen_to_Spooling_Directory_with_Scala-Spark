//////////////////////////////////////////////////////////////////////////////////
//
// File:        DirListener.scala
// Description: Listen_to_Spooling_Directory_with_Scala-Spark, apply analysis to it,
//              then write to multiple files according to specific criterias (customers segmentation criteria)
//              Data is generated by DataGen.jar
// Author:      Mostafa Mamdouh, Radwa Maher
// Created:     Wed Apr 07 13:33:12 PDT 2021
//
//////////////////////////////////////////////////////////////////////////////////


package dirlistener

import org.apache.log4j._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType

object DirListener {

  // Array that will contain paths later on
  var arr: Array[String] = null

  // Set the log level
  Logger.getLogger("org").setLevel(Level.WARN)

  // Start Session
  val spark: SparkSession = SparkSession
    .builder
    .appName("StructuredStreaming")
    .master("local[*]")
    .getOrCreate()

  // Read file
  def readDF (path: String, schemaDef: StructType): DataFrame = {
    spark.read
      .format("csv")
      .option("header", "false")
      .schema(schemaDef)
      .load(path) //.load("data/---.csv")
  }

  // main
  def main(args: Array[String]) {

    // Get four arguments or no arguments and accept default
    if (args.length == 0) { // default arguments
      arr = Array("data\\logs", "data\\SEGMENT.csv", "data\\RULES.csv", "data\\output")
    } else if (args.length != 4) { // throw RuntimeException due to less number of arguments
      throw new RuntimeException("you need four available paths \n" +
        "(spooling dir path, segment file path, rules file path, o/p dir path), for example \n" +
        "java -jar scala-spark-assembly-0.1.jar data\\logs data\\SEGMENT.csv data\\RULES.csv data\\output \n" +
        "or java -jar scala-spark-assembly-0.1.jar, and the default relative paths will the same as above")
    } else { // copy args array
      arr = args.clone()
    }

    // Read Segment file
    val segment = readDF(arr(1), AllSchema.segmentSchema)

    // Read Rule file
    val rule = readDF(arr(2), AllSchema.ruleSchema)

    // Listen to the data directory
    val initData = spark.readStream
      .format("csv")
      .option("header", "false")
      .option("sep", ",")
      .schema(AllSchema.dataSchema)
      .load(arr(0)) //.load("data/logs")

    // From String to TimeStamp & extract hour and date
    val data = initData.withColumn("timestamp",
      to_timestamp(col("time"),"yyyyMMddHHmmss"))
      .withColumn("hour", hour(col("timestamp")))
      .withColumn("date", to_date(col("timestamp")))
      .withColumn("hour_minute", substring(col("time"), 9, 4))

    // Join data & segment
    val dataSegment = data.join(segment, "dial")

    // Join dataSegment & rules
    val dataSegmentRule = dataSegment.join(rule, dataSegment("serviceId") === rule("serviceId2")
      && dataSegment("trafficVol") > rule("trafficLimit")
      && dataSegment("hour") >= rule("startTime")
      && dataSegment("hour") <= rule("endtTime"), "inner")

    // Select desired cols
    val prepareResult = dataSegmentRule.select("dial","serviceId","trafficVol", "time", "timestamp", "date", "hour_minute", "serviceName")

    // Create empty appended dataframe with the same schema
    // It will be mutated later, the only mutation for a data frame in the code to keep history of only one day
    var appendedDateFrame = spark.createDataFrame(spark.sparkContext
      .emptyRDD[Row], prepareResult.schema)

    // For each batch stream
    val updateQuery = prepareResult
      .writeStream
      .outputMode("append")
      .format("console")
      .foreachBatch((batch: DataFrame, batchId: Long) => {
        // Define logic, to take data that is not duplicated in terms of the same day
        val logic = batch.join(appendedDateFrame,batch("serviceId") ===  appendedDateFrame("serviceId")
          && batch("dial") ===  appendedDateFrame("dial")
          && batch("date") === appendedDateFrame("date")
          ,"leftanti")

        // Accumulate to the appendedDateFrame
        // Mutated appendedDateFrame
        appendedDateFrame = appendedDateFrame.unionAll(logic)

        // To prevent OOM error, free the data frame at the end of everyday
        // Mutated appendedDateFrame
        if(appendedDateFrame.filter(col("hour_minute") === "2359").count() > 0) {
          appendedDateFrame = spark.createDataFrame(spark.sparkContext
            .emptyRDD[Row], prepareResult.schema)
        }

        //appendedDateFrame.show()

        // Select only the desired cols
        val finalResult = logic.select("dial","serviceId","trafficVol", "time", "serviceName")

        // Write stream to files
        finalResult.coalesce(1)
          .write
          .partitionBy("serviceName")
          .format("csv")
          .option("checkpointLocation", "checkpoint/")
          .mode("append")
          .save(arr(3)) // .save("data/output/")
      })
      .option("checkpointLocation", "data/checkpoint").start()

    // Wait until we terminate the scripts
    updateQuery.awaitTermination()

    // Stop the session
    spark.stop()
  }
}
